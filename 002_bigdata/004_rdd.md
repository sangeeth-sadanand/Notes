# Distributed processing using RDD.

- PySpark, the Python API for Spark, has a key feature called RDD, which stands for Resilient Distributed Dataset

## What is an RDD?

- Immutable collection: Once created, it canâ€™t be changed.
- Distributed: Spread across multiple nodes in the cluster.
- Fault-tolerant: Can recover from node failures.

## Why use RDDs?

- Parallel Processing: Efficiently handles large-scale data operations.
- In-memory Computing: Speeds up data processing by keeping data in memory.
- Lazy Evaluation: Transforms are not executed until an action is called, optimizing the computation.

## Basic Operations

- **Transformations**: Operations like map(), filter(), and reduceByKey() create a new RDD from an existing one.
- **Actions**: Operations like collect(), count(), and saveAsTextFile() that return a value or save data to external storage.

### Transformations

| Transformation                                            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| map(func)                                                 | Return a new distributed dataset formed by passing each element of the source through a function func.                                                                                                                                                                                                                                                                                                                                                                             |
| filter(func)                                              | Return a new dataset formed by selecting those elements of the source on which func returns true.                                                                                                                                                                                                                                                                                                                                                                                  |
| flatMap(func)                                             | Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).                                                                                                                                                                                                                                                                                                                                               |
| mapPartitions(func)                                       | Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator\<T\> => Iterator\<U\> when running on an RDD of type T.                                                                                                                                                                                                                                                                                                                 |
| mapPartitionsWithIndex(func)                              | Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.                                                                                                                                                                                                                                                                        |
| sample(withReplacement, fraction, seed)                   | Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.                                                                                                                                                                                                                                                                                                                                                                   |
| union(otherDataset)                                       | Return a new dataset that contains the union of the elements in the source dataset and the argument.                                                                                                                                                                                                                                                                                                                                                                               |
| intersection(otherDataset)                                | Return a new RDD that contains the intersection of elements in the source dataset and the argument.                                                                                                                                                                                                                                                                                                                                                                                |
| distinct([numPartitions])                                 | Return a new dataset that contains the distinct elements of the source dataset.                                                                                                                                                                                                                                                                                                                                                                                                    |
| groupByKey([numPartitions])                               | When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable\<V\>) pairs. <br>Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.<br>Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks. |
| reduceByKey(func, [numPartitions])                        | When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.                                                                                                                                                                               |
| aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions]) | When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.                                                                    |
| sortByKey([ascending], [numPartitions])                   | When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.                                                                                                                                                                                                                                                                            |
| join(otherDataset, [numPartitions])                       | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.                                                                                                                                                                                                                                                             |
| cogroup(otherDataset, [numPartitions])                    | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.                                                                                                                                                                                                                                                                                                                           |
| cartesian(otherDataset)                                   | When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).                                                                                                                                                                                                                                                                                                                                                                               |
| pipe(command, [envVars])                                  | Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.                                                                                                                                                                                                                                                                              |
| coalesce(numPartitions)                                   | Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.                                                                                                                                                                                                                                                                                                                                |
| repartition(numPartitions)                                | Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.                                                                                                                                                                                                                                                                                                                       |
| repartitionAndSortWithinPartitions(partitioner)           | Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery.                                                                                                                                                                                                   |

### Actions

| Action                                   | Description                                                                                                                                                                                                                                                                                                                                                                                                  |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| reduce(func)                             | Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.                                                                                                                                                                                                |
| collect()                                | Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.                                                                                                                                                                                                                     |
| count()                                  | Return the number of elements in the dataset.                                                                                                                                                                                                                                                                                                                                                                |
| first()                                  | Return the first element of the dataset (similar to take(1)).                                                                                                                                                                                                                                                                                                                                                |
| take(n)                                  | Return an array with the first n elements of the dataset.                                                                                                                                                                                                                                                                                                                                                    |
| takeSample(withReplacement, num, [seed]) | Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.                                                                                                                                                                                                                                                  |
| takeOrdered(n, [ordering])               | Return the first n elements of the RDD using either their natural order or a custom comparator.                                                                                                                                                                                                                                                                                                              |
| saveAsTextFile(path)                     | Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.                                                                                                                                                  |
| saveAsSequenceFile(path)                 | Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). |
| saveAsObjectFile(path)(Java and Scala)   | Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().                                                                                                                                                                                                                                                                     |
| countByKey()                             | Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.                                                                                                                                                                                                                                                                                                       |
| countByValue()                           | The countByValue function in PySpark is used to count the occurrence of each unique value in an RDD. rdd.countByValue() returns a dictionary where the keys are the unique values from the RDD, and the values are the counts of each unique value.                                                                                                                                                          |
| foreach(func)                            | Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems. <br>Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.                                                                        |

## Parellelize

- `parallelize` is a key function in PySpark used to create an RDD from a list or iterable. 
- It distributes the data across the Spark cluster, allowing for parallel processing.
- This is extremely useful when you need to process large datasets efficiently.
- sc.parallelize(data) distributes the elements of data across the nodes in the cluster, creating an RDD.

```python
rdd = sc.parallelize(data)
```

## Types of Transformations

- Transformations can be categorized into two main types: narrow and wide transformations.
- **Narrow Transformations**
  - Narrow transformations are those where each partition of the parent RDD is used by at most one partition of the child RDD. 
  - These transformations are generally faster and more efficient since they do not require data shuffling across the cluster. 
  - Example: map, filter, flatMap, mapPartitions
- **Wide Transformations**
  - Wide transformations are those where multiple child partitions may depend on one or more partitions of the parent RDD. 
  - These transformations often involve shuffling data across the nodes, which can be more time-consuming and resource-intensive.
  - Example reduceByKey, groupByKey, join, cogroup

## Task, Jobs and Queue

### Tasks

- A task is the smallest unit of work in Spark. 
- Each task represents a single operation applied to a single partition of an RDD. 
- Tasks are executed by executors, which are worker nodes in the Spark cluster.

**Number of task is equal to number of partitions**

### Jobs

- A job is triggered by an action (e.g., collect(), saveAsTextFile(), etc.) and is a sequence of stages. 
- Each job corresponds to a computation (like a chain of transformations followed by an action) that Spark executes to return a result or to write data to external storage.
- **Number of Jobs is equal to number of actions executed**

### Stages

- Stages are intermediate phases of the execution of a job. 
- Spark divides a job into multiple stages based on transformations that can be computed without shuffling data across the nodes.
- Each stage contains tasks that can be executed in parallel on different partitions of the data. 
- Stages are created based on wide transformations, such as reduceByKey or join.
- **Number of Stages is equal to number of wide transformation + 1**

## reduce Vs reduceByKey

- reduce
  
  - The reduce function in PySpark is used to aggregate the elements of an RDD using a specified binary operator. 
  - It's a simple aggregation function that takes a function as an argument and applies it to all elements of the RDD to return a single result.

- reduceByKey
  
  - The reduceByKey function, on the other hand, is used specifically with key-value pair RDDs (i.e., RDDs where each element is a tuple). 
  - This function merges the values for each key using an associative and commutative reduce function. 
  - It is a wide transformation because it involves shuffling data across the nodes.

## Join

- Joining RDDs in PySpark allows you to combine two datasets based on a common key. 
- There are several types of joins you can perform in PySpark, including inner joins, left outer joins, right outer joins, and full outer joins.

### Inner Join

- An inner join returns only the rows that have matching keys in both RDDs.

```python
rdd1 = sc.parallelize([('a', 1), ('b', 2), ('c', 3)]) 
rdd2 = sc.parallelize([('a', 4), ('b', 5), ('d', 6)]) 
joined_rdd = rdd1.join(rdd2)
```

### Left Outer Join

- A left outer join returns all rows from the left RDD, and the matched rows from the right RDD. 
- If no match is found, the result will contain None for columns from the right RDD.

```python
left_outer_join_rdd = rdd1.leftOuterJoin(rdd2)
```

### Right Outer Join

- A right outer join returns all rows from the right RDD, and the matched rows from the left RDD.
- If no match is found, the result will contain None for columns from the left RDD.

```python
right_outer_join_rdd = rdd1.rightOuterJoin(rdd2)
```

### Full Outer Join

- A full outer join returns all rows when there is a match in either left or right RDD. 
- If there is no match, the result will contain None for the columns that do not have a match.

```python
full_outer_join_rdd = rdd1.fullOuterJoin(rdd2)
```

### broadcast join

- A broadcast join in Spark is a type of join that is optimized for cases where one of the datasets is small enough to be efficiently broadcasted to all executor nodes in the cluster. 
- This avoids the need for shuffling data across the network, significantly improving the performance of the join operation.

#### How Broadcast Join Works

- In a broadcast join, Spark sends a copy of the small dataset to every node in the cluster. 
- This way, each executor can join the small dataset with its partition of the large dataset locally, without having to exchange data between nodes.

#### When to Use Broadcast Join

- When one of the datasets is small enough to fit into memory.
- When you want to avoid the overhead of shuffling large amounts of data across the network.

## Repartition and coalesce

- In PySpark, both repartition and coalesce are used to change the number of partitions in an RDD or DataFrame, but they serve slightly different purposes and are optimized for different scenarios

### Repartition

- repartition is used to increase or decrease the number of partitions in an RDD or DataFrame. 
- It involves a full shuffle of the data across the nodes, which can be expensive but is necessary for certain operations that require equal-sized partitions.

**Use Cases:**

- Increasing the number of partitions to parallelize tasks better.
- Decreasing the number of partitions to optimize performance when writing data out to disk.

### Coalesce

- coalesce is used to decrease the number of partitions in an RDD or DataFrame. 
- Unlike repartition, it tries to minimize the amount of data movement by combining existing partitions. 
- This makes it a more efficient operation compared to a full shuffle, but it should only be used when decreasing the number of partitions.

**Use Cases:**

- Reducing the number of partitions to optimize resource usage.
- When writing data out to a single file or fewer files to reduce file fragmentation.

## Cache

- Caching is a powerful feature in Spark that allows you to store an RDD or DataFrame in memory, making future actions on this data much faster. 
- It's especially useful when you have multiple actions to perform on the same dataset, as it avoids recomputing the data for each action.

### Why Use Caching?

- **Performance**: Speeds up iterative algorithms that reuse the same dataset.
- **Resource Efficiency**: Reduces the time and resources spent on recomputing data.
- **Convenience**: Simplifies the code by avoiding the need to recompute transformations manually.

### How to Cache an RDD

- You can cache an RDD by calling the cache method on it. 
- The cached RDD will be stored in memory and reused in subsequent actions.

```python
cached_rdd = rdd.cache()
```

### Storage Levels

- By default, caching stores the RDD in memory. However, Spark allows you to choose different storage levels based on your use case and available resources:
  - MEMORY_ONLY: Stores RDD as deserialized Java objects in memory. This is the default level.
  - MEMORY_AND_DISK: Stores RDD in memory and spills to disk when memory is full.
  - DISK_ONLY: Stores RDD only on disk.
  - MEMORY_ONLY_SER: Stores RDD as serialized Java objects in memory.
  - MEMORY_AND_DISK_SER: Stores RDD as serialized Java objects in memory and spills to disk when memory is full.

```python
cached_rdd = rdd.persist(StorageLevel.MEMORY_AND_DISK)
```

### When to Use Caching

- Reusing the same dataset multiple times within a job.
- Running iterative algorithms like machine learning and graph processing.
- Caching can significantly improve the performance of your Spark applications by reducing the overhead of recomputing the same data.
